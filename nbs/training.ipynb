{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys\n",
    "import fasttext\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "from similarity.cosine import Cosine\n",
    "from similarity.jaccard import Jaccard\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"<PATH TO INDIC NLP Src>\" # eg. \"/../../../indic_nlp_library/src\"\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"<PATH TO INDIC NLP resouces>\" # eg. \"../../../indic_nlp_resources\"\n",
    "\n",
    "import sys\n",
    "import io\n",
    "\n",
    "sys.path.append(format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "import indicnlp\n",
    "from indicnlp import common\n",
    "\n",
    "#common.init()\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
    "\n",
    "from indicnlp import loader\n",
    "#=======Load API=======#\n",
    "loader.load()\n",
    "#=======Load API=======#\n",
    "from indicnlp.script import  phonetic_sim as psim\n",
    "from indicnlp.script import  indic_scripts as isc\n",
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLayer(nn.Module):\n",
    "    def __init__(self, inputDimension, outputDimension ):\n",
    "\n",
    "        super(DeepLayer,self).__init__()\n",
    "\n",
    "        self.inputDimension = inputDimension\n",
    "        self.outputDimension = outputDimension\n",
    "\n",
    "        self.linear1 = nn.Linear(self.inputDimension, self.inputDimension * 2, bias = True)\n",
    "        self.nonLinear1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.inputDimension  * 2, self.outputDimension, bias = True)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "\n",
    "        output = self.linear2(self.nonLinear1(self.linear1(x_in)))\n",
    "        return output\n",
    "    \n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, inputDimension, outputDimension):\n",
    "        super(FFNN,self).__init__()\n",
    "\n",
    "        self.inputDimension = inputDimension\n",
    "        self.outputDimension = outputDimension\n",
    "\n",
    "        self.layer = DeepLayer(self.inputDimension, self.outputDimension)\n",
    "\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def loss(self, x, y):\n",
    "\n",
    "        outputScores = self.layer(x)\n",
    "        loss = self.bce_loss(outputScores, y)\n",
    "\n",
    "        prob_output = self.sigmoid(outputScores)\n",
    "        return loss, prob_output, y\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        outputScores = self.layer(x)\n",
    "        prob_output = self.sigmoid(outputScores)\n",
    "        \n",
    "        return prob_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = Jaccard(2)\n",
    "def jsim(w1, w2):\n",
    "    return jaccard.similarity(w1,w2)\n",
    "\n",
    "cosine = Cosine(2)\n",
    "normalized_levenshtein = NormalizedLevenshtein()\n",
    "\n",
    "def wls(w1, w2):\n",
    "    cosineSim = cosine.similarity(w1,w2)\n",
    "    normSim = normalized_levenshtein.similarity(w1, w2)\n",
    "    finalScore = 0.75*normSim+0.25*cosineSim\n",
    "    return finalScore\n",
    "\n",
    "def phonetic(w1, w2, lang):\n",
    "    charVector = np.zeros(38)\n",
    "    for c in w1:\n",
    "        charVector += isc.get_phonetic_feature_vector(c,\"hi\")\n",
    "    hinVec = charVector/len(w1)\n",
    "    charVector = np.zeros(38)\n",
    "    for c in w2:\n",
    "        charTrans = UnicodeIndicTransliterator.transliterate(c,\"hi\",lang)\n",
    "        charVector += isc.get_phonetic_feature_vector(charTrans,lang)\n",
    "    tgtlangVec = charVector/len(w2)\n",
    "    sim = psim.cosine(hinVec,tgtlangVec)\n",
    "#     print(hinVec, tgtlangVec)\n",
    "    return sim\n",
    "\n",
    "def phoneticV(w1, w2, lang):\n",
    "    charVector = np.zeros(38)\n",
    "    for c in w1:\n",
    "        charVector += isc.get_phonetic_feature_vector(c,\"hi\")\n",
    "    hinVec = charVector/len(w1)\n",
    "    charVector = np.zeros(38)\n",
    "    for c in w2:\n",
    "        charTrans = UnicodeIndicTransliterator.transliterate(c,\"hi\",lang)\n",
    "        charVector += isc.get_phonetic_feature_vector(charTrans,lang)\n",
    "    tgtlangVec = charVector/len(w2)\n",
    "    sim = psim.cosine(hinVec,tgtlangVec)\n",
    "#     print(hinVec, tgtlangVec)\n",
    "    return hinVec, tgtlangVec\n",
    "\n",
    "def crossVec(w1, w2, himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang):\n",
    "    if w1 in hiDict:\n",
    "        hi_index = hiDict[ w1 ]\n",
    "        hiVec = hiCrossModel[ hi_index ]\n",
    "    else:\n",
    "        hiVec = himodel.get_word_vector(w1)\n",
    "    \n",
    "    if w2 in tgtDict:\n",
    "        tgt_index = tgtDict[ w2 ]\n",
    "        tgtVec = tgtCrossModel[ tgt_index ]\n",
    "    else:\n",
    "        tgtVec = tgtmodel.get_word_vector(w2)\n",
    "    return hiVec, tgtVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation for Similarity scores or Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData(file, simType, himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang):\n",
    "    X = []\n",
    "    y = []\n",
    "    with io.open(file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            word_pairs = []\n",
    "            line = line.strip()\n",
    "            line = line.split(\";\")\n",
    "#             word1, word2, con1, con2, label, g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, g12, g13, g14, g15, g16, g17, g18 = line[3].strip(), line[4].strip(), line[5].strip(), line[6].strip(), line[25].strip(), line[7].strip(), line[8].strip(), line[9].strip(), line[10].strip(), line[11].strip(), line[12].strip(), line[13].strip(), line[14].strip(), line[15].strip(), line[16].strip(), line[17].strip(), line[18].strip(), line[19].strip(), line[20].strip(), line[21].strip(), line[22].strip(), line[23].strip(), line[24].strip()\n",
    "            word1, word2, con1, con2, label, g1, g2, g3, g4, g5, g6, g7, g8 = line[2].strip(), line[3].strip(), line[4].strip(), line[5].strip(), line[14].strip(), line[6].strip(), line[7].strip(), line[8].strip(), line[9].strip(), line[10].strip(), line[11].strip(), line[12].strip(), line[13].strip()\n",
    "#             gaze_list = [float(g1), float(g2), float(g3), float(g4), float(g5), float(g6), float(g7), float(g8), float(g9), float(g10), float(g11), float(g12), float(g13), float(g14), float(g15), float(g16), float(g17), float(g18)]\n",
    "            gaze_list = [float(g1), float(g2), float(g3), float(g4), float(g5), float(g6), float(g7), float(g8)]\n",
    "            if(simType == \"JSim\"):\n",
    "                wSim = jsim(word1, word2)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                score = 0.0\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        score = score + jsim(conWord1, conWord2)\n",
    "                conSim = score / (len(con1)*len(con2)*1.0)\n",
    "                finalList = [wSim, conSim]\n",
    "            elif(simType == \"WLS\"):\n",
    "                wSim = wls(word1, word2)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                score = 0.0\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        score = score + wls(conWord1, conWord2)\n",
    "                conSim = score / (len(con1)*len(con2)*1.0)\n",
    "                finalList = [wSim, conSim]\n",
    "            elif(simType == \"WLSGaze\"):\n",
    "                wSim = wls(word1, word2)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                score = 0.0\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        score = score + wls(conWord1, conWord2)\n",
    "                conSim = score / (len(con1)*len(con2)*1.0)\n",
    "#                 finalList = [wSim, conSim, float(g1), float(g2), float(g3), float(g4), float(g5), float(g6), float(g7), float(g8)]\n",
    "                finalList = [wSim, conSim, float(g1), float(g2), float(g3), float(g4), float(g5), float(g6), float(g7), float(g8), float(g9), float(g10), float(g11), float(g12), float(g13), float(g14), float(g15), float(g16), float(g17), float(g18)]\n",
    "            elif(simType == \"Phonetic\"):\n",
    "                wSim = phonetic(word1, word2, lang)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                score = 0.0\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        score = score + phonetic(conWord1, conWord2, lang)\n",
    "                conSim = score / (len(con1)*len(con2)*1.0)\n",
    "                finalList = [wSim, conSim]\n",
    "            elif(simType == \"PhoneticGaze\"):\n",
    "                wSim = phonetic(word1, word2, lang)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                score = 0.0\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        score = score + phonetic(conWord1, conWord2, lang)\n",
    "                conSim = score / (len(con1)*len(con2)*1.0)\n",
    "#                 finalList = [wSim, conSim, float(g1), float(g2), float(g3), float(g4), float(g5), float(g6), float(g7), float(g8)]\n",
    "                finalList = [wSim, conSim, float(g1), float(g2), float(g3), float(g4), float(g5), float(g6), float(g7), float(g8), float(g9), float(g10), float(g11), float(g12), float(g13), float(g14), float(g15), float(g16), float(g17), float(g18)]    \n",
    "            elif(simType == \"PhoneticV\"):\n",
    "                vec1, vec2 = phoneticV(word1, word2, lang)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                fcVec1 = np.zeros(38)\n",
    "                fcVec2 = np.zeros(38)\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        cVec1, cVec2 = phoneticV(conWord1, conWord2, lang)\n",
    "                        fcVec1 += cVec1\n",
    "                        fcVec2 += cVec2\n",
    "                fcVec1 = fcVec1 / (len(con1)*1.0)\n",
    "                fcVec2 = fcVec2 / (len(con2)*1.0)\n",
    "                finalList = np.concatenate((vec1, vec2, fcVec1, fcVec2))\n",
    "            elif(simType == \"PhoneticVGaze\"):\n",
    "                vec1, vec2 = phoneticV(word1, word2, lang)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                fcVec1 = np.zeros(38)\n",
    "                fcVec2 = np.zeros(38)\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        cVec1, cVec2 = phoneticV(conWord1, conWord2, lang)\n",
    "                        fcVec1 += cVec1\n",
    "                        fcVec2 += cVec2\n",
    "                fcVec1 = fcVec1 / (len(con1)*1.0)\n",
    "                fcVec2 = fcVec2 / (len(con2)*1.0)\n",
    "#                 finalGaze = np.array(gaze_list).reshape(8,)\n",
    "                finalGaze = np.array(gaze_list).reshape(18,)\n",
    "                finalList = np.concatenate((vec1, vec2, fcVec1, fcVec2, finalGaze))\n",
    "            elif(simType == \"monolingual\"):\n",
    "                vec1, vec2 = monolingual(word1, word2, himodel, tgtmodel)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                fcVec1 = np.zeros(50)\n",
    "                fcVec2 = np.zeros(50)\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        cVec1, cVec2 = monolingual(conWord1, conWord2, himodel, tgtmodel)\n",
    "                        fcVec1 += cVec1\n",
    "                        fcVec2 += cVec2\n",
    "                fcVec1 = fcVec1 / (len(con1)*1.0)\n",
    "                fcVec2 = fcVec2 / (len(con2)*1.0)\n",
    "                finalList = np.concatenate((vec1, vec2, fcVec1, fcVec2))\n",
    "            elif(simType == \"crossSim\"):\n",
    "                finalList = [s1, s2, s3, s4]\n",
    "            elif(simType == \"crossVec\"):\n",
    "#                 print(\"Started preparing data for Hi-\"+lang+\"!\")\n",
    "                vec1, vec2 = crossVec(word1, word2, himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                fcVec1 = np.zeros(50)\n",
    "                fcVec2 = np.zeros(50)\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        cVec1, cVec2 = crossVec(conWord1.strip(), conWord2.strip(), himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang)\n",
    "                        cVec1 = cVec1.astype(float)\n",
    "                        cVec2 = cVec2.astype(float)\n",
    "                        fcVec1 += cVec1\n",
    "                        fcVec2 += cVec2\n",
    "                fcVec1 = fcVec1 / (len(con1)*1.0)\n",
    "                fcVec2 = fcVec2 / (len(con2)*1.0)\n",
    "                finalList = np.concatenate((vec1, vec2, fcVec1, fcVec2))\n",
    "#                 print(finalList)\n",
    "            elif(simType == \"crossVecGaze\"):\n",
    "                writeFile2 = io.open\n",
    "#                 print(\"Started preparing data for Hi-\"+lang+\"!\")\n",
    "                vec1, vec2 = crossVec(word1, word2, himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang)\n",
    "                con1 = con1.split()\n",
    "                con2 = con2.split()\n",
    "                fcVec1 = np.zeros(50)\n",
    "                fcVec2 = np.zeros(50)\n",
    "                for conWord1 in con1:\n",
    "                    for conWord2 in con2:\n",
    "                        cVec1, cVec2 = crossVec(conWord1.strip(), conWord2.strip(), himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang)\n",
    "                        cVec1 = cVec1.astype(float)\n",
    "                        cVec2 = cVec2.astype(float)\n",
    "                        fcVec1 += cVec1\n",
    "                        fcVec2 += cVec2\n",
    "                fcVec1 = fcVec1 / (len(con1)*1.0)\n",
    "                fcVec2 = fcVec2 / (len(con2)*1.0)                \n",
    "                finalGaze = np.array(gaze_list).reshape(8,)\n",
    "#                 finalGaze = np.array(gaze_list).reshape(18,)\n",
    "                finalList = np.concatenate((vec1, vec2, fcVec1, fcVec2, finalGaze))\n",
    "                \n",
    "            X.append(finalList)\n",
    "            y.append(int(label))\n",
    "#     print(len(X), len(y))\n",
    "    X = np.asarray(X, dtype='float32')\n",
    "    y = np.asarray(y, dtype='int32')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    dictionary = dict()\n",
    "    reverseDict = []\n",
    "\n",
    "    wv = []\n",
    "    dimension = 0\n",
    "    with codecs.open(file_name, 'r', 'utf-8',errors='ignore') as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if len(line.split(' ')) == 2:\n",
    "                    continue\n",
    "\n",
    "                vocabulary = line.split(' ')[0]\n",
    "                if vocabulary.lower() not in dictionary:\n",
    "                    temp = []\n",
    "                    dictionary[vocabulary.lower()] = len(dictionary)\n",
    "                    reverseDict.append(vocabulary.lower())\n",
    "\n",
    "                    if dimension == 0:\n",
    "                        dimension = len(line.split(' ')[1:])\n",
    "\n",
    "                    if dimension != len(line.split(' ')[1:]):\n",
    "                        print(line)\n",
    "                        print(str(dimension) +\"\\t\" + str(len(line.split(' ')[1:])))\n",
    "                        exit()\n",
    "                    for i in line.split(' ')[1:]:\n",
    "                        temp.append(float(i))\n",
    "\n",
    "                    wv.append(temp)\n",
    "\n",
    "    wordEmbedding = np.array(wv)\n",
    "\n",
    "    return wordEmbedding, dictionary, reverseDict, wordEmbedding.shape[0], dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable1, iterable2, n=1):\n",
    "    l = iterable1.shape[0]\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable1[ndx:min(ndx + n, l)], iterable2[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['mr']\n",
    "# methods = ['FF', 'SVM', 'SVMP', 'SVMG', 'LR']\n",
    "methods = ['FF']\n",
    "sims = [\"crossVec\"]\n",
    "\n",
    "with io.open(\"EACL_CrossVec_Results.csv\", \"w+\") as outfile:\n",
    "    outfile.write(\"Language Pair;Similarity;Method;C;Folds;Precision;Recall;F-Score\\n\")\n",
    "    for lang in langs:\n",
    "        himodel = fasttext.load_model(\"../fastText-0.2.0/models/himodel50.bin\")\n",
    "        tgtmodel = fasttext.load_model(\"../fastText-0.2.0/models/\"+lang+\"model50.bin\")\n",
    "        hiCrossModel, hiDict, hiRevDict, NS, embDim = load_embeddings(\"../data/MUSE/hi\"+lang+\"/vectors-hi.txt\")\n",
    "        tgtCrossModel, tgtDict, tgtRevDict, tgtNS, tgtembDim = load_embeddings(\"../MUSE/hi\"+lang+\"/vectors-\"+lang+\".txt\")\n",
    "        for sim in sims:\n",
    "            for mlmethod in methods:\n",
    "                X,y = prepData(\"../data/D1/D1.csv\", sim, himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang)\n",
    "                vals = [1e-2, 1e-1, 1]\n",
    "                for c in vals:\n",
    "                    n = 5\n",
    "                    skf = StratifiedKFold(n_splits=n, random_state=42)\n",
    "                    skf.get_n_splits(X, y)\n",
    "                \n",
    "                    fprecision = 0\n",
    "                    frecall = 0\n",
    "                    ffscore = 0\n",
    "                    for train_index, test_index in skf.split(X, y):\n",
    "                    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                        \n",
    "                        X_train, X_test = X[train_index], X[test_index]\n",
    "                        y_train, y_test = y[train_index], y[test_index]\n",
    "                        if(mlmethod == \"SVM\"):\n",
    "                            svclassifier = SVC(kernel='linear', C=c)\n",
    "                        elif(mlmethod == \"SVMP\"):\n",
    "                            svclassifier = SVC(kernel='poly', C=c)\n",
    "                        elif(mlmethod == \"SVMG\"):\n",
    "                            svclassifier = SVC(kernel='rbf', C=c)\n",
    "                        elif(mlmethod == \"LR\"):\n",
    "                            svclassifier = LogisticRegression(C=c)\n",
    "                        elif(mlmethod == \"FF\"):\n",
    "                            ff_nn = FFNN(X_train.shape[1], 1)\n",
    "                            ff_nn.cuda()\n",
    "                            initial_lr = 0.4\n",
    "                            optim = SGD(ff_nn.parameters(), lr=initial_lr, momentum=0.04, weight_decay=0.0, nesterov=True)\n",
    "                  \n",
    "                        if(mlmethod == 'FF'):\n",
    "                            for epoch in range(1, 20):\n",
    "                                for inputs, labels in batch(X_train, y_train, 20):\n",
    "                                    input_tensor = torch.from_numpy(inputs).cuda()\n",
    "                                    output_tensor = torch.FloatTensor(labels).cuda().unsqueeze(1)\n",
    "                                    \n",
    "                                    optim.zero_grad()\n",
    "                                    loss, _, _ = ff_nn.loss(input_tensor, output_tensor)\n",
    "\n",
    "                                    loss.backward()\n",
    "                                    optim.step()\n",
    "                                    \n",
    "                                lrate = initial_lr * np.exp(-0.1 * epoch)\n",
    "                                optim = SGD(ff_nn.parameters(), lr=lrate, momentum=0.04, weight_decay=0.0, nesterov=True)\n",
    "\n",
    "                            \n",
    "                            ff_nn.eval()\n",
    "                            true = []\n",
    "                            predicted = []\n",
    "                            \n",
    "                            for inputs, labels in batch(X_test, y_test, 20):\n",
    "                                input_tensor = torch.from_numpy(inputs).cuda()\n",
    "                                \n",
    "                                true_labels = []\n",
    "                                for i in range(labels.shape[0]):\n",
    "                                    true_labels.append(int(labels[i]))\n",
    "                                true.extend(true_labels)\n",
    "\n",
    "                                optim.zero_grad()\n",
    "                                pred_labels = ff_nn.forward(input_tensor)\n",
    "                                pred_output = pred_labels.data.cpu().numpy()\n",
    "                                pred = []\n",
    "\n",
    "                                for i in range(len(pred_output)):\n",
    "                                    if pred_output[i] >= 0.5:\n",
    "                                        pred.append(1)\n",
    "                                    else:\n",
    "                                        pred.append(0)\n",
    "                                predicted.extend(pred)\n",
    "                            \n",
    "                            precision = precision_score(true, predicted, average='weighted')\n",
    "                            recall = recall_score(true, predicted, average='weighted')\n",
    "                            fscore = f1_score(true, predicted, average='weighted')\n",
    "                            fprecision  = fprecision + precision\n",
    "                            frecall = frecall + recall\n",
    "                            ffscore = ffscore + fscore\n",
    "                \n",
    "                        else:\n",
    "                            svclassifier.fit(X_train, y_train)\n",
    "                            y_pred = svclassifier.predict(X_test)\n",
    "                            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                            fscore = f1_score(y_test, y_pred, average='weighted')\n",
    "                            fprecision  = fprecision + precision\n",
    "                            frecall = frecall + recall\n",
    "                            ffscore = ffscore + fscore\n",
    "                    fprecision = fprecision / n\n",
    "                    frecall = frecall / n\n",
    "                    ffscore = ffscore / n\n",
    "                    outfile.write(\"hi-\"+lang+\";\"+sim+\";\"+mlmethod+\";\"+str(c)+\";\"+str(n)+\";\"+str(fprecision)+\";\"+str(frecall)+\";\"+str(ffscore)+\"\\n\")\n",
    "#                     print(\"hi-\"+lang+\";\"+sim+\";\"+mlmethod+\";\"+str(c)+\";\"+str(n)+\";\"+str(fprecision)+\";\"+str(frecall)+\";\"+str(ffscore))\n",
    "                    outfile.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['mr']\n",
    "# methods = ['FF', 'SVM', 'SVMP', 'SVMG', 'LR']\n",
    "methods = ['SVM', 'LR']\n",
    "sims = [\"crossVecGaze\"]\n",
    "\n",
    "with io.open(\"CoNLLCrossVecGazePredictionResults.csv\", \"w+\") as outfile:\n",
    "    outfile.write(\"Language Pair;Similarity;Method;C;Folds;Precision;Recall;F-Score\\n\")\n",
    "    for lang in langs:\n",
    "        himodel = fasttext.load_model(\"../fastText-0.2.0/models/himodel50.bin\")\n",
    "        tgtmodel = fasttext.load_model(\"../fastText-0.2.0/models/\"+lang+\"model50.bin\")\n",
    "        hiCrossModel, hiDict, hiRevDict, NS, embDim = load_embeddings(\"../MUSE/hi\"+lang+\"/vectors-hi.txt\")\n",
    "        tgtCrossModel, tgtDict, tgtRevDict, tgtNS, tgtembDim = load_embeddings(\"../MUSE/hi\"+lang+\"/vectors-\"+lang+\".txt\")\n",
    "        for sim in sims:\n",
    "            for mlmethod in methods:\n",
    "                X,y = prepData(\"../data/D2/D2.csv\", sim, himodel, tgtmodel, hiCrossModel, hiDict, tgtCrossModel, tgtDict, lang)\n",
    "                vals = [1e-2, 1e-1, 1]\n",
    "                for c in vals:\n",
    "                    n = 5\n",
    "                    skf = StratifiedKFold(n_splits=n, random_state=42)\n",
    "                    skf.get_n_splits(X, y)\n",
    "                \n",
    "                    fprecision = 0\n",
    "                    frecall = 0\n",
    "                    ffscore = 0\n",
    "                    for train_index, test_index in skf.split(X, y):\n",
    "                    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                        \n",
    "                        X_train, X_test = X[train_index], X[test_index]\n",
    "                        y_train, y_test = y[train_index], y[test_index]\n",
    "                        if(mlmethod == \"SVM\"):\n",
    "                            svclassifier = SVC(kernel='linear', C=c)\n",
    "                        elif(mlmethod == \"SVMP\"):\n",
    "                            svclassifier = SVC(kernel='poly', C=c)\n",
    "                        elif(mlmethod == \"SVMG\"):\n",
    "                            svclassifier = SVC(kernel='rbf', C=c)\n",
    "                        elif(mlmethod == \"LR\"):\n",
    "                            svclassifier = LogisticRegression(C=c)\n",
    "                        elif(mlmethod == \"FF\"):\n",
    "                            ff_nn = FFNN(X_train.shape[1], 1)\n",
    "                            ff_nn.cuda()\n",
    "                            initial_lr = 0.4\n",
    "                            optim = SGD(ff_nn.parameters(), lr=initial_lr, momentum=0.04, weight_decay=0.0, nesterov=True)\n",
    "                  \n",
    "                        if(mlmethod == 'FF'):\n",
    "                            for epoch in range(1, 20):\n",
    "                                for inputs, labels in batch(X_train, y_train, 20):\n",
    "                                    input_tensor = torch.from_numpy(inputs).cuda()\n",
    "                                    output_tensor = torch.FloatTensor(labels).cuda().unsqueeze(1)\n",
    "                                    \n",
    "                                    optim.zero_grad()\n",
    "                                    loss, _, _ = ff_nn.loss(input_tensor, output_tensor)\n",
    "\n",
    "                                    loss.backward()\n",
    "                                    optim.step()\n",
    "                                    \n",
    "                                lrate = initial_lr * np.exp(-0.1 * epoch)\n",
    "                                optim = SGD(ff_nn.parameters(), lr=lrate, momentum=0.04, weight_decay=0.0, nesterov=True)\n",
    "\n",
    "                            \n",
    "                            ff_nn.eval()\n",
    "                            true = []\n",
    "                            predicted = []\n",
    "                            \n",
    "                            for inputs, labels in batch(X_test, y_test, 20):\n",
    "                                input_tensor = torch.from_numpy(inputs).cuda()\n",
    "                                \n",
    "                                true_labels = []\n",
    "                                for i in range(labels.shape[0]):\n",
    "                                    true_labels.append(int(labels[i]))\n",
    "                                true.extend(true_labels)\n",
    "\n",
    "                                optim.zero_grad()\n",
    "                                pred_labels = ff_nn.forward(input_tensor)\n",
    "                                pred_output = pred_labels.data.cpu().numpy()\n",
    "                                pred = []\n",
    "\n",
    "                                for i in range(len(pred_output)):\n",
    "                                    if pred_output[i] >= 0.5:\n",
    "                                        pred.append(1)\n",
    "                                    else:\n",
    "                                        pred.append(0)\n",
    "                                predicted.extend(pred)\n",
    "                            \n",
    "                            precision = precision_score(true, predicted, average='weighted')\n",
    "                            recall = recall_score(true, predicted, average='weighted')\n",
    "                            fscore = f1_score(true, predicted, average='weighted')\n",
    "                            fprecision  = fprecision + precision\n",
    "                            frecall = frecall + recall\n",
    "                            ffscore = ffscore + fscore\n",
    "                \n",
    "                        else:\n",
    "                            svclassifier.fit(X_train, y_train)\n",
    "                            y_pred = svclassifier.predict(X_test)\n",
    "                            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                            fscore = f1_score(y_test, y_pred, average='weighted')\n",
    "                            fprecision  = fprecision + precision\n",
    "                            frecall = frecall + recall\n",
    "                            ffscore = ffscore + fscore\n",
    "                    fprecision = fprecision / n\n",
    "                    frecall = frecall / n\n",
    "                    ffscore = ffscore / n\n",
    "                    outfile.write(\"hi-\"+lang+\";\"+sim+\";\"+mlmethod+\";\"+str(c)+\";\"+str(n)+\";\"+str(fprecision)+\";\"+str(frecall)+\";\"+str(ffscore)+\"\\n\")\n",
    "#                     print(\"hi-\"+lang+\";\"+sim+\";\"+mlmethod+\";\"+str(c)+\";\"+str(n)+\";\"+str(fprecision)+\";\"+str(frecall)+\";\"+str(ffscore))\n",
    "                    outfile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
